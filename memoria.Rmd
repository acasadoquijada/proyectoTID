---
title: "proyectoTID"
author: "Alejandro Casado Quijada y Gustavo Rivas Gervillas"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arules)
library(plyr)
library(ggplot2)
library(sampling)
```

## Introducción

## Descripción del dataset

Este dataset contiene datos recogidos de la aplicacion *PokemonGo*, esta aplicación es un juego de realidad aumeta que emplea el GPS del móvil para principalmente localizar y capturar pokemon en el mundo real. El dataset contiene 296021 muestras cada una de las cuales dispone de los siguientes campos:

* **pokemonId**: el identificador del pokemon, denota su clase.
* **latitude**: latitud de la posición donde se ha localizado el pokemon.
* **longitude**: longitud de la posición donde se ha localizado el pokemon.
* **appearedLocalTime**: momento exacto en el que se encontró el pokemon, con el formato yyyy-mm-ddThh-mm-ss.ms.
* **X_id**: la ficha del dataset no proporciona información sobre qué representa este dato, de hecho hemos [preguntado en la propia web](https://www.kaggle.com/semioniy/predictemall/discussion/24061#144095) pero no hemos obtenido respuesta. No obstante viendo el dataset no parece ser más que una código identificador de la muestra. 
* **cellId 90-5850m**: la localización goegráfica del pokemon proyectada en una celda S2.
* **appearedTimeOfDay**: momento del día en el que apareció el pokemon (night, evening, afternoon, morning).
* **appearedHour**: hora local de una observación del pokemon.
* **appearedMinute**: minuto local de una observación del pokemon.
* **appearedDayOfWeek**: día de la semana en la que se produjo el avistamiento (Monday, Tuesday, Wednesday, Thursday, Friday, Saturdy, Sunday).
* **appearedDay**: día del avistamiento.
* **appearedMonth**: mes del avistamiento.
* **appearedYear**: año del avistamiento.
* **terrainType**: tipo del terreno donde se avistó el pokemon. Este dato viene dado por un valor número según una [tabla de tipos de terreno](http://glcf.umd.edu/data/lc/).
* **closeToWater**: si está el pokemon a 100m del agua o no.
* **city**: ciudad donde se ha visto el pokemon.
* **continent**: continente donde se ha avistado el pokemon.
* **weather**: un string indicando el tiempo que hacía en el momento del avistamiento.
* **temperature**: temperatura en grados Celsius en el momento del avistamiento.
* **windSpeed**: velocidad del viento en el momento del avistamiento km/h.
* **windBearing**: dirección del viento entre 0 y 360 grados.
* **pressure**: presión en el momento del avistamiento en bares.
* **weatherIcon**: el tiempo atmosférico en el momento del avistamiento clasificado según un sistema de categorías más simple que el empleado en *weather* (fog, clear-night, partly-cloudy-night, partly-cloudy-day, cloudy, clear-day, rain, wind).
* **sunriseMinutesMidnight**: tiempo de la aparición relativo al amanecer.
* **sunsetMinutesBefore**: tiempo de la aparición relativo a la puesta de sol.
* **population density**: densidad de población por $km^2$ en un avistamiento.
* **urbal-rural**: cómo de urbana es la localización donde apareció el pokemon relativa a la *population density* (<200 rural, >= 200 && < 400 midUrban, >= 400 && < 800 subUrban, >800 urban).
* **gymDistanceKm**: distancia al gimnasio más cercano al punto de aparición del pokemon.
* **pokestopDistanceKm**: distancia a la pokestop más cercana al punto de aparicion del pokemon.
* **gymIn100m** - **pokestopIn5000m**: son atributos booleanos que indican si hay un gimnasio o una pokestop a 100m/250m/500m/1000m/2500m/5000m de la localización donde se avistó el pokemon.
* **cooc1** - **cooc151**: booleano que indica si el avistamiento de un pokemon coincidió con el de otro (de una clase entre 1 y 151) en un radio de 100m y en un rango de tiempo de 24 horas.
* **class** dice qué pokemon se trata, y en la página del dataset indica que es el atributo a predecir.
## Preprocesamiento

En primer lugar vamos a ver cuántas muestras y atributos tiene nuestro dataset. Además veremos si las clases están balanceadas, para echo emplearemos el comando `xtab`:

```{r}
ds <- read.csv("300k.csv")
```

```{r}
cat("Hay un total de " , nrow(ds) , " muestras.\n")
cat("Cada muestra tiene " , ncol(ds) , " atributos.\n")
```

Vamos a proceder a eliminar el atributo **appearedLocalTime**, la eliminamos en primer lugar por la dificultad de trabajar con este dado, el cual podríamos transformar en una serie de variables que desglosasen su contenido, no obstante tenemos otros atributos que ya lo hacen, como son la hora, el día, el mes y el año del avistamiento. Por otro lado vamos a eleminar también el atributo **X_id** que como ya hemos comentado no sabemos qué representa. Además teniendo en cuenta que la aplicación se lanzó el día 6 de julio de 2016, es claro que no el año del avistamiento no aporta ninguna información, con lo que también eleminaremos el atributo **appearedYear**.

```{r}
ds <- subset(ds, select = -c(appearedLocalTime, X_id, appearedYear))
```

Viendo el dataset nos hemos dado cuenta de que para los atributos booleanos que nos indican si hay un gimnasio o una pokeparada a una distancia determinada del lugar de avistamiento del pokemon, siguen un patrón, y es que, al parecer, estas variables lo que indican es si hay un gimnasio o una pokeparada **en un radio** de una determinada longitud, con lo cual en cuanto el atributo que indica si hay un gimansio a una distancia es cierto, el resto de atributos que indican si hay un gimnasio a una distancia mayor también lo son. Lo mismo sucede con las paradas. Entonces una vez hayamos confirmado esto, como la existencia de un gimnasio o una pokeparada en un radio determinado implica la existencia en un radio mayor, podremos eliminar, sin pérdida de información, todos estos atributos y quedarnos únicamente con los atributos **gymDistanceKm** y **pokestopDistanceKm**, que resumirían la información contenida en los otros atributos. Para tratar de comprobar esto vamos a hacer uso de las reglas de asociación con el paquete *arules*:

```{r}
booleansGyms = subset(ds, select = c(gymIn100m, gymIn250m, gymIn500m, gymIn1000m, gymIn2500m, gymIn5000m))
reglas = apriori(booleansGyms, parameter = list(support = 0.0, confidence = 0.8, minlen = 2, maxlen = 2))
inspect(subset(reglas, confidence == 1.0))

booleansPokestops = subset(ds, select = c(pokestopIn100m, pokestopIn250m, pokestopIn500m, pokestopIn1000m, pokestopIn2500m, pokestopIn5000m))
reglas = apriori(booleansPokestops, parameter = list(support = 0.0, confidence = 0.8, minlen = 2, maxlen = 2))
inspect(subset(reglas, confidence == 1.0))
```

Hemos obtenido las distintas reglas de asociación sin atender al soporte ya que no estamos interesados en saber cuantas veces se da una correspondencia entre dos hechos (el soporte de dicha regla), lo que queremos saber es que cuando se da un hecho, esto implica que se den los que suponemos que deberian darse (la confianza de las reglas encontradas). Por tanto, como podemos ver, las reglas de asociación encontradas prueban nuestras suposiciones. Dado esto, procedemos a elminar los atributos. Dado que la confianza es del 100%, no afecta haber usado el conjunto entero.

```{r}
ds <- subset(ds, select = -c(gymIn100m, gymIn250m, gymIn500m, gymIn1000m, gymIn2500m, gymIn5000m, pokestopIn100m, pokestopIn250m, pokestopIn500m, pokestopIn1000m, pokestopIn2500m, pokestopIn5000m))
```
Este dataset nos proporciona una serie de atributos para la localización del pokemon. Los primeros que nos encontramos son **latitude** y **longitude**, se tratan de coordenadas geográficas. Por otro lado aparecen **cellId_90m**, **cellId_180m**, **cellId_370m**, **cellId_730m**, **cellId_1460m**, **cellId_2920m**, **cellId_5850m**. Estos indican la posición geográfica usando celdas s2. Estas celdas se clasifican en niveles atendiendo a su área, desde 0 (menor área) a 30 (mayor área). Se obtienen según longitud y latitud, por lo que son la misma información representada de distinta manera. Además para métodos que dependen de una distancia como el KNN tendríamos que investigar la distancia entre las distintas celdas a través de su ID, lo que supondría una carga de trabajo extra e innecesaria. Se puede consultar más información sobre las celdas s2 en el siguiente [enlace](http://blog.christianperone.com/2015/08/googles-s2-geometry-on-the-sphere-cells-and-hilbert-curve/)

Por lo comentando arriba se va a optar a eliminar los atributos correspondientes a las celdas, dejando los atributos **latitude** y **longitude** como los únicos para determinar la posición.

```{r}
ds <- subset(ds, select = -c(cellId_90m, cellId_180m, cellId_370m, cellId_730m, cellId_1460m, cellId_2920m, cellId_5850m))
```

TODO: explicación de por qué quitamos la ciudad y el continente:

```{r}
ds <- subset(ds, select = -c(city, continent))
```

Por otro lado al explorar el dataset observamos que en el atributo **appearedDayOfWeek** se tomaba el valor *dummy_day*, y al consultar los valores que toma este atributo a lo largo del dataset vimos que no aparecia el lunes cuando el resto de los dias de la semana si que aparecen, con lo cual procedemos a revisar si este valor del atributo se corresponde con el lunes o es simplemente un valor perdido:

```{r}
unique(ds$appearedDayOfWeek)
unique(subset(ds, appearedDayOfWeek == "dummy_day")$appearedMonth)
unique(subset(ds, appearedDayOfWeek == "dummy_day")$appearedDay)
```

Como podemos observar el único día en el que se registran observaciones en las que el atributo toma el valor *dummy_day* es el 8 de agosto que efectivamente fue un lunes. Además mientras se comprobaba este hecho hemos observado que todas las observaciones se realizaron en agosto y que además fue durante una semana de agosto, con lo cual el atributo **appearedMonth** no aporta ninguna información y además los atributos **appearedDayOfWeek** y **appearedDay** aportan la misma información, ya que al tomarse las muestras durante una sola semana hay una correspondencia biyectiva entre los valores de ambos atributos. Y como a priori no consideramos que la distancia entre días sea significativa, vamos a optar por quedarnos con el atributo categórico:

```{r}
#le damos un categoría más singnificativa que dummy_day al atributo
levels(ds$appearedDayOfWeek)[1] <- "Monday"

#vemos los meses y días en los que se tomaron las muestras
unique(ds$appearedMonth)
unique(ds$appearedDay)

#eliminamos las variables innecesarias
ds <- subset(ds, select = -c(appearedMonth, appearedDay))
```

Podemos comprobar también que podemos obtener los atributos **sunriseMinutesMidnight** y **sunsetMinutesMidnight** a partir de los atributos: **sunriseHour**, **sunriseMinute**, **sunsetHour** y **sunsetMinute**, con lo que procedemos a eliminar estos 4 últimos:

```{r}
sum(ds$sunriseMinutesMidnight == (ds$sunriseHour*60 + ds$sunriseMinute))/nrow(ds)*100
sum(ds$sunsetMinutesMidnight == (ds$sunsetHour*60 + ds$sunsetMinute))/nrow(ds)*100
ds <- subset(ds, select = -c(sunriseHour, sunriseMinute, sunsetHour, sunsetMinute))
```

Con lo cual como vemos el número de muestras y de atributos es muy elevado. Además el número de clases a clasificar es muy elevado por lo tanto lo que vamos a hacer es intentar clasificar los pokemon según su tipo, así que vamos a añadir dicha columna al dataset. Los tipos considerados son aquellos que se establecieron en la primera generación de Pokemon y se recogen por medio de las siguientes variables que almacenan una cadena indicando el tipo; estas variables se han añadido por comodidad a la hora de generar el vector de tipos que crearemos a continuación:

```{r}
P = "planta"
A = "agua"
F = "fuego"
B = "bicho"
G = "fantasma"
L = "lucha"
N = "normal"
E = "electrico"
S = "psiquico"
V = "veneno"
H = "hielo"
D = "dragon"
T = "tierra"
R = "roca"
```


A continuación formamos un array considerando el tipo primario de cada pokemon de la primera generación, dando lugar a un array de 151 elementos. Pese que en la app hay algunas especies de pokemon que no aparecen por motivos de indexación, ya que las muestras dan la clase según el número original del pokemon, introducimos el tipo de los 151 pokemon:

```{r}
tipos = c(P,P,P,F,F,F,A,A,A,B,B,B,
  B,B,B,N,N,N,N,N,N,N,V,V,
  E,E,T,T,V,V,V,V,V,V,N,N,
  F,F,N,N,V,V,P,P,P,B,B,B,
  B,T,T,N,N,A,A,L,L,F,F,A,
  A,A,S,S,S,L,L,L,P,P,P,A,
  A,R,R,R,F,F,A,A,E,E,N,N,
  N,A,A,V,V,A,A,G,F,F,R,S,
  S,A,A,E,E,P,P,T,T,L,L,N,
  V,V,T,T,N,P,N,A,A,A,A,A,
  A,S,B,H,E,F,B,N,A,A,A,N,
  N,A,E,F,N,R,R,R,R,R,N,H,
  E,F,D,D,D,S,S)

nombres = c("Bulbasaur", "Ivysaur", "Venusaur", "Charmander", "Charmeleon", "Charizard", "Squirtle", "Wartortle", "Blastoise", "Caterpie", "Metapod", "Butterfree", "Weedle", "Kakuna", "Beedrill", "Pidgey", "Pidgeotto", "Pidgeot", "Rattata", "Raticate", "Spearow", "Fearow", "Ekans", "Arbok", "Pikachu", "Raichu", "Sandshrew", "Sandslash", "NidoranH", "Nidorina", "Nidoqueen", "NidoranM", "Nidorino", "Nidoking", "Clefairy", "Clefable","Vulpix", "Ninetales", "Jigglypuff", "Wigglytuff", "Zubat", "Golbat", "Oddish", "Gloom", "Vileplume", "Paras", "Parasect", "Venonat", "Vanomoth", "Diglett", "Dugtrio", "Meowth", "Persian", "Psyduck", "Golduck", "Mankey", "Primeape", "Growlithe", "Arcanine", "Poliwag", "Poliwhirl", "Poliwrath", "Abra", "Kadabra", "Alakazam", "Machop", "Machoke", "Machamp", "Bellsprout", "Weepinbell", "Victreebel", "Tentacool", "Tentacruel", "Geodude", "Graveler", "Golem", "Ponyta", "Rapidash", "Slowpoke", "Slowbro", "Magnemite", "Magneton", "Farfetch'd", "Doduo", "Dodrio", "Seel", "Dewgong", "Grimer", "Muk", "Shellder", "Cloyster", "Gastly", "Haunter", "Gengar", "Onix", "Drowzee", "Hypno", "Krabby", "Kingler", "Voltorb", "Electrode", "Exeggcute", "Exeggutor", "Cubone", "Marowak", "Hitmonlee", "Hitmonchan", "Lickitung", "Koffing", "Weezing", "Rhyhorn", "Rhydon", "Chansey", "Tangela", "Kangaskhan", "Horsea", "Seadra", "Goldeen", "Seaking", "Staryu", "Starmie", "Mr. Mime", "Scyther", "Jynx", "Electrabuzz", "Magmar", "Pinsir", "Tauros", "Magikarp", "Gyarados", "Lapras", "Ditto", "Evee", "Vaporeon", "Jolteon", "Flareon", "Porygon", "Omanyte", "Omastar", "Kabuto", "Kabutops", "Aerodactyl", "Snorlax", "Articuno", "Zapdos", "Moltres", "Dratini", "Dragonair", "Dragonite", "Mewtwo", "Mew")
```

Ahora vamos a proceder a añadir la columna de tipos al dataset que hemos cargado, así como el nombre del pokemon observado:

```{r}
ds["tipo"] <- tipos[ds$class]
ds["nombre"] <- nombres[ds$class]
```

Veamos entonces cómo se distribuyen las muestras que tenemos según el tipo de pokemon avistado:

```{r}
T <- xtabs(~ tipo, ds)
etiquetas = c("agua", "bicho", "dragon", "electrico", "fantasma", "fuego", "hielo", "lucha", "normal", "planta", "psiquico", "roca", "tierra", "veneno")
print(T)
colores = rev(c("darkorchid", "burlywood3","darkgoldenrod4", "pink", "forestgreen", "darkgray", "brown4", "cyan2", "red", "midnightblue", "yellow", "slateblue4", "yellowgreen", "dodgerblue"))
barplot(T, main = "Distribución de tipos", horiz = TRUE, ylab= "Tipo", col = colores, legend.text = etiquetas, args.legend=list(bty = "n", x = "right"))
```

Como podemos ver una vez que hemos agrupado las muestras por el tipo de Pokemon vemos que las clases están tremendamente desequilibradas. Mientras que hay muchísimos avistamientos de Pokemon de tipo normal, las clases hielo, dragón, fastasma, eléctrico o roca resultan marginales. Con lo cual en primer lugar hemos de realizar un equilibrado de estas clases.

Dado que el número de clases es muy grande hemos, aún habiendo agrupado por tipo de pokemon las muestras, hemos optado por quedarnos sólo con 5 clases. La elección ha sido quedarnos con las clases agua, fuego, planta, psiquíco y fantasma.

```{r}

#reglasGenerales <- apriori(ds[, -c(1, 2, 3, 5, 6, 8, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 182, 183, 184)], parameter = list(support = 0.1, confidence = 0.8, minlen = 2, maxlen = 3, maxtime=300))
```
```{r}
#inspect(reglasGenerales)
```
```{r}
#unique(ds[ds$appearedDayOfWeek=="Sunday",]$pokemonId)
```

```{r}
ds$appearedTimeOfDay <- factor(ds$appearedTimeOfDay, levels = c("morning", "afternoon", "evening", "night"))
tabla <- table(ds$appearedTimeOfDay, ds$tipo)
tabla1 <- table(ds[! ds$tipo %in% c("normal", "bicho", "agua", "dragon", "hielo", "veneno"), c("appearedTimeOfDay", "tipo")])
tabla2 <- table(ds[ds$tipo %in% c("normal", "bicho", "agua", "veneno"), c("appearedTimeOfDay", "tipo")])
tabla3 <- table(ds[ds$tipo %in% c("dragon","hielo"), c("appearedTimeOfDay", "tipo")])
barplot(tabla1, legend = levels(ds$appearedTimeOfDay), las = 2, col = c("darkgoldenrod2", "chocolate", "cyan4", "blue4"))
barplot(tabla2, legend = levels(ds$appearedTimeOfDay), las = 2, col = c("darkgoldenrod2", "chocolate", "cyan4", "blue4"))
barplot(tabla3, legend = levels(ds$appearedTimeOfDay), las = 2, col = c("darkgoldenrod2", "chocolate", "cyan4", "blue4"))
```

Aquí podemos apreciar que para cualquier tipo de pokemon la mayoría de los avistamientos se producen durante la noche. Creíamos que ibamos a apreciar una tendencia en la que los pokemon de tipo fantasma fuesen los que presentaban este comportamiento de una forma más acusada, sin embargo todos los tipos siguen este patrón de aparición.

```{r}
unosDatos <- ds[ds$nombre == "Abra" | ds$nombre == "Kadabra",]
tabla <- table(unosDatos$nombre, unosDatos$population_density)
plot(unosDatos$population_density, type = "l")
```

Una de las principales características de Pokemon GO, aplicación de la cual son estos datos, es la posibilidad de encontrar pokemon en cualquier parte, por ejemplo en entornos cercanos al agua. Se espera, como es natural, que la mayoría de pokemon tipo agua se encuentren en zonas cercanas al agua. Esto daría un gran grado de realismo a la aplicación. Vamos a comprobar si esto es cierto con el tipo agua y a su vez con algunos otros. Para ello vamos a realizar una sere de gráficos.

```{r}
tabla3 <- table(ds$closeToWater, ds$tipo)
barplot(tabla3, legend = levels(ds$closeToWater), las = 2, col = c("burlywood4", "deepskyblue3"), horiz = TRUE)

conteo <- merge(count(ds[ds$closeToWater == "true",]$tipo), count(ds$tipo), by = "x")
colnames(conteo) <- c("tipo", "cercaDelAgua", "total")
conteo["proporcion"] <- conteo$cercaDelAgua / conteo$total
conteo[with(conteo, order(-proporcion)), ]
```
Mostramos tanto una gráfica en la que se puede ver la distribución de las muestras referentes a cada tipo según se hayan avistado cerca del agua o no y un ranking ordenado según la proporción de avistamientos cerca del agua. Así vemos que los pokemon de tipo eléctrico, dragón, agua e hielo son los que aparecen en mayor proporción cerca del agua. Por otro lado los pokemon de tipo fuego, planta, roca, normal y bicho son los que en menor proporción se presentan en zona acuosas. Nos sorprende de este análisis dos cosas: que no sean los pokemon de tipo agua los que sean más habituales en proporción en zonas acuosas y que los pokemons de tipo planta estén en menor proporción en las zonas con agua que en las zonas secas.

Aquí estamos hablando de proporciones, si nos fijamos en cantidades sí son los pokemon (después de los normales) de tipo agua los que aparecen más veces cerca del agua, no obstante preferimos atender a las proporciones ya que no queremos hay que tener en cuenta que habrá unos tipo de pokemon más difíciles de encontrar que otros.

Antes hemos estado discutiendo sobre los dias de las apariciones de los diferentes pokemon. Por lo que ahora vamos a ver como se distribuyen dichas apariciones a lo largo de los dias de la semana. Para realizar esto vamos a elegir a los siguientes tipos: psíquico, planta, fuego, tierra y roca

```{r}
ds$appearedDayOfWeek <- factor(ds$appearedDayOfWeek, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
conteoDias <- table(ds$appearedDayOfWeek, ds$tipo)
barplot(conteoDias, las = 2, col = c("gray8", "gray27", "gray41", "gray58","green","green4","forestgreen"), legend = levels(ds$appearedDayOfWeek))

diasSemana <- c("Monday", "Tuesday", "Wednesday", "Thursday")
tablaSemana <- count(ds[ds$appearedDayOfWeek %in% diasSemana, "tipo"])
tablaFinDeSemana <- count(ds[! ds$appearedDayOfWeek %in% diasSemana, "tipo"])
conteoSemana <- merge(tablaSemana, tablaFinDeSemana, by = "x")
colnames(conteoSemana) <- c("tipo", "L-J", "V-D")
conteoSemana
```

Podemos observar que una gran parte de los avistamientos se realizan en el fin de semana, concretamente los sábados y viernes. También se producen grandes avistamientos los miércoles y jueves, mientras que el lunes es el día que menos avistamientos se producen. Todo esto tiene sentido, ya que hemos de pensar que los datos que estamos tratando son de un video juego para móviles, por lo tanto la mayor actividad, avistamientos, se realizarán cuando los jugadores dispongan de mayor tiempo libre para jugar, fin de semana.

Dado que contamos con la latitud y longitud de los avistamiento decidimos representarlos en un mapa, pero al pintar únicamente los puntos referentes a avistamientos en Madrid nos encontramos lo siguiente:

```{r}
world_map <- map_data("world")
base_world <- ggplot() + coord_fixed() + xlab("") + ylab("") + geom_polygon(data=world_map, aes(x=long, y=lat, group=group), colour="lightsalmon4", fill="burlywood3")
mapa <- base_world + geom_point(data = ds[ds$nombre == "Tauros",], aes(x = longitude, y = latitude))
mapa
```

Hay puntos que no se sitúan en Madrid, otros que sí. Pensamos que podría deberse a un problema con el sistema de coordenadas empleado en el datates y el empleado en el mapa base. Entonces realizamos el siguiente experimento:

```{r}
minLat <- min(ds[ds$city == "Madrid",]$latitude)
maxLat <- max(ds[ds$city == "Madrid",]$latitude)
minLong <- min(ds[ds$city == "Madrid",]$longitude)
maxLong <- max(ds[ds$city == "Madrid",]$longitude)

#Veamos a qué ciudades pertenecen los avistamientos cuya latitud y longitud están en el rango de los puntos de Madrid
unique(ds[ds$latitude >= minLat & ds$latitude <= maxLat & ds$longitude >= minLong & ds$longitude <= maxLong,]$city)
```

Como podemos ver hay avistamientos dentro del mismo rango de latitud y longitud que los puntos en Madrid en ciudades tan dispares como Londres, Berlín, Nueva York o Sydney en Australia. Por tanto consideramos que el problema es que al haber formado el dataset, como comenta el propio creador del mismo, con datos de distintas fuentes, las coordeandas de estos avistamientos provienen de distintos sistemas de coordenadas, produciendo este tipo de inconsistencias. Por tanto estos datos juntos con los referentes a los ids de las celdas S2 de distinto tamaño que se obtienen, según se dice en la página principal del dataset, a partir de ellas son datos erróneos y que no se pueden usar para nuestro estudio. No obstante consideraremos que el atributo referencia a la ciudad del avistamiento sí que puede ser correcto y lo mantendremos para nuestro estudio.

```{r}
madrid <- ds[ds$city == "Madrid",]

raro <- ds[ds$nombre == "Tauros",]
unique(raro$city)
unique(raro$continent)
```


DISTRIBUCIÓN de los tipos en el mapa.

```{r}
world_map <- map_data("world")
base_world <- ggplot() + coord_fixed() + xlab("") + ylab("") + geom_polygon(data=world_map, aes(x=long, y=lat, group=group), colour="lightsalmon4", fill="burlywood3")
colores <- rev(c("darkorchid", "burlywood3","darkgoldenrod4", "pink", "forestgreen", "darkgray", "brown4", "cyan2", "red", "midnightblue", "yellow", "slateblue4", "yellowgreen", "dodgerblue"))
escaleta <- scale_colour_manual(name = c("agua", "bicho", "dragon", "electrico", "fantasma", "fuego", "hielo", "lucha", "normal", "planta", "psiquico", "roca", "tierra", "veneno"), values = colores)
mapa <- base_world + geom_point(data = ds, aes(x = longitude, y = latitude, color = ds$tipo)) + escaleta
mapa
```

Distribución de los tipos de terreno en el mapa

```{r}
world_map <- map_data("world")
base_world <- ggplot() + coord_fixed() + xlab("") + ylab("") + geom_polygon(data=world_map, aes(x=long, y=lat, group=group), colour="lightsalmon4", fill="burlywood3")
mapa <- base_world + geom_point(data = ds, aes(x = longitude, y = latitude, colour = ds$terrainType))
mapa
```

Distribución de los distintos tipos de clima en el mapa

```{r}
world_map <- map_data("world")
base_world <- ggplot() + coord_fixed() + xlab("") + ylab("") + geom_polygon(data=world_map, aes(x=long, y=lat, group=group), colour="lightsalmon4", fill="burlywood3")
mapa <- base_world + geom_point(data = ds, aes(x = longitude, y = latitude, colour = ds$weather))
mapa
```




Debido a la gran cantidad de muestras y clases del dataset, hemos decidido quedarnos con 5 clases, 5 pokemon, de aproximadamente 1000 muestras cada una para hacer clasificación TODO explicar por qué nos centramos en un problema de clasificación y no en otro:

```{r}
T <- xtabs(~ nombre, ds)
T[nombre = "Exeggcute"]
T[nombre = "Squirtle"]
T[nombre = "Pinsir"]
T[nombre = "Meowth"]
T[nombre = "Kakuna"]

nombrePokemonSelecionados = c("Exeggcute", "Squirtle", "Pinsir", "Meowth","Kakuna")
dsFiltrado = ds[ds$nombre %in% nombrePokemonSelecionados,]

write.csv(dsFiltrado,file = "5Pokemon.csv",row.names = FALSE)
```

Hemos decidido quedarnos con los pokemon Exeggcute (1786), Squirtle (1490), Pinsir (1404), Meowth(1757) y Kakuna(1807) TODO explicar por qué elegimos estas clases y no otras.

```{r}
dsFiltrado <- read.csv("5Pokemon.csv")
```

Eliminamos los atributos **pokemonId** y **class** ya que este atributo es el que es objetivo de nuestra clasificación:

```{r}
dsFiltrado <- subset(dsFiltrado, select = -c(pokemonId, class, tipo))
```

Lo que vamos a hacer es buscar variables que tengan al menos dos valores distintos a lo largo del dataset. Las que solo presenten un valor serán eliminadas ya que no aportan ninguna información para la clasificación.

```{r}
indicesAquitar = (apply(dsFiltrado, 2, function(x)length(unique(x))) == 1)
dsFiltrado <- dsFiltrado[, !indicesAquitar]
```

En un primer uso del dataset para probar los árboles de decisión nos encontramos con que una variable factor tenía más de 32 niveles con lo que los árboles no podían trabajar con nuestro dataset. Al revisar los tipos de variable del dataset nos encontramos con la el atributo **pokestopDistanceKm** era de tipo factor. Esto no tiene sentido ya que realmente esta variable es simplemente una variable que nos da la distancia en kilómetros del avistamiento de un pokemon a una poke-parada. Por lo tanto pasamos esta variable a numérica:

```{r}
dsFiltrado$pokestopDistanceKm = as.numeric(as.character(dsFiltrado$pokestopDistanceKm))
```
Al pasar esta variable a numérica observámos algo que se nos había pasado por alto, revisar si teníamos valores perdido en nuestro dataset, entonces observamos que esto se daba en una sola muestra, con lo que decidimos quitarla:

```{r}
idx <- which(is.na(dsFiltrado$pokestopDistanceKm))
dsFiltrado <- dsFiltrado[-idx,]
```

Y puesto que esto nos ha pasado con este atributo pasamos a ver si esto se daba en alguno más. Comprobamos que no se daba:

```{r}
sum(apply(dsFiltrado, 2, function(x) sum(is.na(x))) >= 1)
```


Antes de proceder al uso de árboles de decisión vamos a realizar una normalización de las distintas variables numéricas, esto lo hacemos ya que, entre otros algoritmos de clasificación, vamos a usar el KNN el cuál tiene su base en la distancia entre muestras, por lo tanto queremos evitar que la escala de un atributo haga que éste tenga más peso que otros en las decisiones del algoritmo:

```{r}

normalize <- function(x) {
    return ((x - min(x)) / (max(x) - min(x)))
}

numericIdx <- sapply(dsFiltrado, is.numeric)
dsFiltrado[, numericIdx] = apply(dsFiltrado[, numericIdx], 2, normalize)
```

Como a partir del atributo **population_density** podemos obtener los 4 atributos **urban**, **suburban**, **midurban** y **rural**, ya que lo que queremos hacer con este nuevo dataset es clasificación, vamos a quitarlos:

```{r}
dsFiltrado = subset(dsFiltrado, select = -c(urban, suburban, midurban, rural))
```

Ya hemos dicho anteriormente que vamos a trabajar con algunos algoritmos que dependen de la distancia entre las muestras, entonces lo que vamos a hacer ahora es pasar aquellos atributos categóricos con dos niveles (que son false y true) a atributos numéricos con dos valores 0 y 1. Estos atributos son los relativos a las coocurrencias y el que nos indica si el avistamiento fue próximo al agua, **closeToWater**:

```{r}
a <- which(colnames(dsFiltrado) == "cooc_1")
b <- which(colnames(dsFiltrado) == "cooc_149")
c <- which(colnames(dsFiltrado) == "closeToWater")
binaryIdx = c(a:b, c)
dsFiltrado[, binaryIdx] = apply(dsFiltrado[, binaryIdx], 2, function(x) ifelse(x == "true", 1, 0))
```

Nuevamente debido a la influencia de la distancia vamos a pasar los atributos categóricos con más de 2 niveles (salvo el relativo al nombre del pokemon y el tipo ya que no los usaremos en la clasificación) a atributos binarios, uno por cada nivel de cada uno de estros atributos. Para ello haremos uso de la función `dummyVars` del paquete **caret**:

```{r}
library(caret)

dsFiltrado <- cbind(dsFiltrado, predict(dummyVars(~ appearedTimeOfDay, data = dsFiltrado), newdata = dsFiltrado))
dsFiltrado <- cbind(dsFiltrado, predict(dummyVars(~ appearedDayOfWeek, data = dsFiltrado), newdata = dsFiltrado))
dsFiltrado <- cbind(dsFiltrado, predict(dummyVars(~ weather, data = dsFiltrado), newdata = dsFiltrado))
dsFiltrado <- cbind(dsFiltrado, predict(dummyVars(~ weatherIcon, data = dsFiltrado), newdata = dsFiltrado))

dsFiltrado <- subset(dsFiltrado, select = -c(appearedTimeOfDay, appearedDayOfWeek, weather, weatherIcon))
```

Al intentar usar la función tree para obtener un árbol de decisión obteniamos un error diciéndo que teníamos una variable categórica con más de 32 nivels, usando el código `str(dsFiltrado)`, observamos que el atributo pokestopDistanceKm era categórico, algo que no conisderamos que tenga mucho sentido. Con lo cual pasamos a transformar este atributo a numérico:
```{r}
idx <- which(colnames(dsFiltrado) == "weatherIcon.clear-day")
colnames(dsFiltrado)[idx] <- "weatherIcon.clearDay"
idx <- which(colnames(dsFiltrado) == "weatherIcon.clear-night")
colnames(dsFiltrado)[idx] <- "weatherIcon.clearNight"
idx <- which(colnames(dsFiltrado) == "weatherIcon.partly-cloudy-day")
colnames(dsFiltrado)[idx] <- "weatherIcon.partlyCloudlyDay"
idx <- which(colnames(dsFiltrado) == "weatherIcon.partly-cloudy-night")
colnames(dsFiltrado)[idx] <- "weatherIcon.partlyCloudlyNight"
```

como podemos ver no parece haber más valores perdido con los que procedemos a estudiar algunas técnicas de clasificación sobre este dataset. Comenzaremos con los árboles de decisión, en primer lugar lo que haremos es obtener dos particiones de entrenamiento y test. En una primera aproximación vamos a realizar simplemente un submuestreo aleatorio sin más; luego mejoraremos este procedimiento para ver cómo afecta la calidad de las particiones a los resultados de clasificación que obtengamos:

```{r}
set.seed(42)

dsFiltradoClaseNum <- rep(0, nrow(dsFiltrado))
dsFiltradoClaseNum[dsFiltrado$nombre == "Kakuna"] <- 1
dsFiltradoClaseNum[dsFiltrado$nombre == "Meowth"] <- 2
dsFiltradoClaseNum[dsFiltrado$nombre == "Pinsir"] <- 3
dsFiltradoClaseNum[dsFiltrado$nombre == "Squirtle"] <- 4

#dsFiltrado$nombre = dsFiltradoClaseNum
idx = sample(2, 0.7*nrow(dsFiltrado), replace = TRUE, prob = c(0.7, 0.3))
c(sample(which(dsFiltrado$nombre == "Pinsir"), 1000), sample(which(dsFiltrado$nombre == "Kakuna"), 1000), sample(which(dsFiltrado$nombre == "Meowth"), 1000), sample(which(dsFiltrado$nombre == "Squirtle"), 1000), sample(which(dsFiltrado$nombre == "Exeggcute"), 1000)) -> trainIdx
train = dsFiltrado[trainIdx,]
test = dsFiltrado[-trainIdx,]
```

```{r}
library(tree)
arbol = tree(nombre ~ ., data = train)
plot(arbol) ; text(arbol)
testPredict = predict(arbol, newdata = test, type = "class")
tt = table(testPredict, test$nombre)
cat("El porcentaje de acierto en test con un árbol de decisión hecho con tree:",sum(diag(tt))/nrow(test)*100)
```

```{r}
library(party)

arbol = ctree(nombre ~ ., data = train)
plot(arbol, type="simple")
```

```{r}
testPredict = predict(arbol, newdata = test)
tt = table(testPredict, test$nombre)
dd = diag(tt)
cat("El porcentaje de acierto con un arbol de decisión hecho con party es:",sum(dd)/nrow(test)*100)
```

```{r}
#rpart, rattle, RColorBrewer
library(rpart)
library(rpart.plot)

arbol = rpart(nombre ~ ., data = train)
plot(arbol) ; text(arbol)
rpart.plot(arbol)
testPredict = predict(arbol, newdata = test, type = "class")
tt = table(testPredict, test$nombre)
dd = diag(tt)
cat("El porcentaje de acierto con un árbol de decisión hecho con rpart es:", sum(dd)/nrow(test)*100)
```

Dado los resultados obtenidos utilizando los m?todos de clasificaci?n vamos a proceder a utilizar **random forest**


```{r}
library(randomForest)
library(data.table)
library(ggplot2)

modelo = nombre ~.
arbol = randomForest(modelo, data = train)

table(predict(arbol),train$nombre)
arbol

# Get OOB data from plot and coerce to data.table
oobData = as.data.table(plot(arbol))

# Define trees as 1:ntree
oobData[, trees := .I]

# Cast to long format
oobData2 = melt(oobData, id.vars = "trees")
setnames(oobData2, "value", "error")

# Plot using ggplot
ggplot(data = oobData2, aes(x = trees, y = error, color = variable)) + geom_line()


testpred=predict(arbol,newdata=test)
table(testpred,test$nombre)
tt=table(testpred,test$nombre)

dd=diag(tt)
bien=sum(dd)
bien_clasificados=(bien/nrow(test))*100
bien_rf=bien_clasificados
#Bien y mal clasificados
cat("Porcentaje de acierto con un randomForest:", bien_clasificados)
100-bien_clasificados

#Precision, Recall y F-measure
ft=0 
m=c(1:nrow(tt))
n=m
prec=n
rec=m
fmes=m
for(i in 1:nrow(tt)){m[i]=sum(tt[i,])}
for(i in 1:nrow(tt)){n[i]=sum(tt[,i])}
for(i in 1:nrow(tt)){prec[i]=dd[i]/m[i]}
for(i in 1:nrow(tt)){rec[i]=dd[i]/n[i]} 
for(i in 1:nrow(tt)){fmes[i]=2*dd[i]/(m[i]+n[i])}
ft=0
for(i in 1:nrow(tt)){ft=ft+(fmes[i]/nrow(tt))}
#Precision
prec

#Recall
rec
#F-measure
fmes
#F-measure total
ft
ft_rf=ft 

```


Pruebas con KNN

```{r}
library(kknn)

modelo = nombre ~.

arbol.kknn=kknn(formula=modelo, train, test, na.action=na.omit(),k=3)
fit=fitted(arbol.kknn)
tt=table(fit,test$nombre)
tt

dd=diag(tt)
bien=sum(dd)
bien_clasificados=(bien/nrow(test))*100

bien_knn=bien_clasificados
cat("Porcentaje de bien clasificados con knn:", bien_clasificados)
```

```{r}
library(e1071)

mknns = tune.knn(dsFiltrado[, -135], dsFiltrado$nombre, k=1:20, tunecontrol = tune.control(sampling = "cross"), cross = 10)
```

Con el código anterior hemos obtenido que el mejor k de entre 1 y 20 para el algoritmo de KNN en nuestro dataset es 9, probemos este parámetro:

```{r}
modelo = nombre ~.

arbol.kknn=kknn(formula=modelo, train, test, na.action=na.omit(),k=9)
fit=fitted(arbol.kknn)
tt=table(fit,test$nombre)
tt

dd=diag(tt)
bien=sum(dd)
bien_clasificados=(bien/nrow(test))*100

bien_knn=bien_clasificados
cat("Porcentaje de bien clasificados con knn:", bien_clasificados)
```

Aumenta ligeramente aunque no demasiado. No sabemos si esto se debe a emplear un paquete distinto al que pertenece tune.knn:

```{r}
library(xgboost)
trainIndex <- createDataPartition(iris$Species, p = .8, list = FALSE, times = 1)
idxNombre = which(colnames(dsFiltrado) == "nombre")
xgb <- xgboost(data = data.matrix(dsFiltrado[trainIndex, -idxNombre]), label = dsFiltradoClaseNum[trainIndex], eta = 0.1, max_depth = 7, nround=25, subsample = 0.5, colsample_bytree = 0.5, seed = 1, eval_metric = "merror", objective = "multi:softmax", num_class = 5, nthread = 3)
```

exeggcute = 0
kakuna = 1
meowth = 2
metapod = 3
squirtle = 4

```{r}
y_pred <- predict(xgb, data.matrix(dsFiltrado[-trainIndex, -idxNombre]))
sum(dsFiltradoClaseNum[-trainIndex] == y_pred)/nrow(dsFiltrado)*100
```

